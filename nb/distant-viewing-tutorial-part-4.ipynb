{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Viewing with Deep Learning: Part 4\n",
    "\n",
    "We further extend our techniques to working with moving images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Load the Distant Viewing Library\n",
    "\n",
    "The following code will load the distant viewing library; if\n",
    "you do not have the library installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if importlib.util.find_spec(\"dvt\") is not None:\n",
    "    import dvt\n",
    "    dvt_flag = True\n",
    "else:\n",
    "    dvt_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Location of objects\n",
    "\n",
    "The distant viewing toolkit can be used to detect objects within\n",
    "an image. We start by creating an object detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(\"..\", \"data\", \"bewitched.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    odrn = dvt.annotate.object.ObjectDetectRetinaNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find objects in the following images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = join('..', 'images', 'bewitched', df.filename[200])\n",
    "img = imread(img_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we detect objects in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    objs = odrn.detect(img)\n",
    "else: \n",
    "    with open (join('..', 'cache', 'bw_face_ex.pickle'), 'rb') as fp:\n",
    "        objs = pickle.load(fp)\n",
    "        \n",
    "objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the faces, we can show these within the image. We'll add\n",
    "some labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "plt.imshow(img)\n",
    "n, m, d = img.shape\n",
    "for obj in objs:\n",
    "    rect = plt.Rectangle((obj['left'], obj['top']),\n",
    "                          obj['right'] - obj['left'],\n",
    "                          obj['bottom'] - obj['top'],\n",
    "                         edgecolor='orange', linewidth=2, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    plt.text(obj['left'], obj['top'] - 12, obj['class'],\n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='orange'))\n",
    "    \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: DVT Demo\n",
    "\n",
    "The real power the distant viewing toolkit is to analyze moving images. \n",
    "We are going to look at a very short clip of an episode of Friends. Let's\n",
    "load in the functions that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    from dvt.annotate.core import FrameProcessor, FrameInput, ImageInput\n",
    "    from dvt.annotate.diff import DiffAnnotator\n",
    "    from dvt.annotate.face import FaceAnnotator, FaceDetectDlib, FaceEmbedVgg2\n",
    "    from dvt.annotate.meta import MetaAnnotator\n",
    "    from dvt.annotate.png import PngAnnotator\n",
    "    from dvt.aggregate.cut import CutAggregator\n",
    "\n",
    "    import logging\n",
    "    logging.basicConfig(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by constructing a frame input object attached to the video file. The bsize argument indicates that we will work with the video by looking through batches of 128 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    finput = FrameInput(join(\"..\", \"video\", \"bewitched.mp4\"), bsize=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a frame processor and add four annotators: (i) metadata, (ii) png files, (iii) differences between successive frames, and (iv) faces. The quantiles input to the DiffAnnotator indicates that we want to compute the 40th percentile in differences between frames. The face detector take a long time to run when not on a GPU, so we restrict it to running only every 64 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    fpobj = FrameProcessor()\n",
    "    fpobj.load_annotator(PngAnnotator(output_dir=join(\"..\", \"video-clip-frames\")))\n",
    "    fpobj.load_annotator(MetaAnnotator())\n",
    "    fpobj.load_annotator(DiffAnnotator(quantiles=[40]))\n",
    "    fpobj.load_annotator(FaceAnnotator(detector=FaceDetectDlib(), freq=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the pipeline of annotators over the input object. We will turn on logging here to see the output as Python processes each annotator over a batch of frames. The max_batch argument restricts the number of batches for testing purposes; set to None (default) to process the entire video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    fpobj.process(finput, max_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is now stored in the fpobj object. To access it, we call its collect_all method. This method returns a dictionary of custom objects (DictFrame, an extension of an ordered dictionary). Each can be converted to a Pandas data frame for ease of viewing the output or saving as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    obj = fpobj.collect_all()\n",
    "    lobj = dict()\n",
    "\n",
    "    for k in obj.keys():\n",
    "        lobj[k] = obj[k].todf()\n",
    "    \n",
    "else:  \n",
    "    with open (join('..', 'cache', 'friends_dvt_ex.pickle'), 'rb') as fp:\n",
    "        lobj = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (join('..', 'cache', 'friends_dvt_ex.pickle'), 'rb') as fp:\n",
    "        lobj = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at each output type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "The metadata is not very exciting, but is useful for downstream tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobj['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Png\n",
    "\n",
    "The png annotator does not return any data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobj['png']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, its used for its side-effects. You will see that there are individual frames from the video now saved in the directory \"video-clip-frames\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference\n",
    "\n",
    "The difference annotator indicates the differences between successive frames, as well as information about the average value (brightness) of each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobj['diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face\n",
    "\n",
    "The face annotator detects faces in the frames. We configured it to only run every 64 frames, so there is only output in frames 0, 64, 128, and 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobj['face']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are two faces in frame 0, 64, and 192 but four faces detected in frame 128. In fact, all six of the main cast members are in frame 128, but two are two small and obscured to be found by the dlib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect cuts\n",
    "\n",
    "We can also aggregate the information to detect cuts in the video file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    from dvt.aggregate.cut import CutAggregator\n",
    "    cagg = CutAggregator(cut_vals={'q40': 3})\n",
    "    cout = cagg.aggregate(obj).todf()\n",
    "else:\n",
    "    with open (join('..', 'cache', 'friends_dvt_cut_ex.pickle'), 'rb') as fp:\n",
    "        cout = pickle.load(fp)\n",
    "\n",
    "cout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you should see that these correspond with the cuts in the input video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvt.annotate.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvt.annotate.core.ImageInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: DVT with Still Images\n",
    "\n",
    "Finally, it is also possible to use the pipeline API with a collection of\n",
    "still images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dvt_flag:\n",
    "    finput = ImageInput(join(\"..\", \"images\", \"bewitched\", \"*\"))\n",
    "    \n",
    "    fpobj = FrameProcessor()\n",
    "    fpobj.load_annotator(FaceAnnotator(detector=FaceDetectDlib()))\n",
    "    \n",
    "    fpobj.process(finput, max_batch=20)\n",
    "    \n",
    "    obj = fpobj.collect_all()\n",
    "    lobj = dict()\n",
    "\n",
    "    for k in obj.keys():\n",
    "        lobj[k] = obj[k].todf()\n",
    "        \n",
    "else:  \n",
    "    with open (join('..', 'cache', 'bw_dvt_ex.pickle'), 'rb') as fp:\n",
    "        lobj = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobj['face']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
